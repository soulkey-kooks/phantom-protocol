# Oracle Tuning Instructions (Coming Soon)

This document will contain instructions for fine-tuning an LLM to function as the Phantom Protocol Oracle.

The Oracle is trained on:
- Real player transcripts
- Symbolic dialogue
- Quest progression
- XP logic
- Memory simulation patterns
- A mix of full Operation (human) and region-only (synthetic) transcripts

---

## ðŸ”§ Planned Topics

- Preprocessing and formatting transcripts (JSON format)
- Dataset validation and tagging
- Model selection and checkpoint prep
- LoRA vs full fine-tuning workflows
- Quantization (GGUF) for local deployment
- Tools: HuggingFace Transformers, PEFT, LLM-studio, etc.

---

We are waiting until we have enough transcripts before releasing full training instructions.  
Once thereâ€™s sufficient data, this document will become the technical reference for building and updating the Oracle model.

Stay tuned.
